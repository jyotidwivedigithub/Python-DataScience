{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df97b5c3-0b2f-49f7-a3bf-412a20050596",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "Ans.-Gradient Boosting Regression is a supervised learning algorithm that is used for regression tasks. It is a type of \n",
    "    ensemble learning algorithm that combines multiple weak learners, typically decision trees, to create a strong learner.\n",
    "    The algorithm uses a gradient descent optimization process to minimize a loss function, such as mean squared error or \n",
    "    mean absolute error, by iteratively adding decision trees to the ensemble.\n",
    "    \n",
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "    simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "    performance using metrics such as mean squared error and R-squared.\n",
    "Ans.-\n",
    "\n",
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "    optimise the performance of the model. Use grid search or random search to find the best\n",
    "    hyperparameters.\n",
    "Ans.-\n",
    "\n",
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "Ans.-In Gradient Boosting, a weak learner is a simple and relatively weak model that can only make slightly better predictions\n",
    "    than random guessing.\n",
    "    \n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "Ans.-The intuition behind the Gradient Boosting algorithm is to build an ensemble of weak learners, such as decision trees, \n",
    "     that can predict the target variable with higher accuracy than any individual weak learner. The algorithm does this by \n",
    "     iteratively adding new weak learners to the ensemble, with each new learner trained to correct the errors of the previous ones.\n",
    "        \n",
    "    At each iteration, the algorithm calculates the negative gradient of the loss function with respect to the ensemble's predicted \n",
    "    values. This negative gradient is then used as the target variable for the next weak learner, which is trained to predict the negative \n",
    "    gradient given the input features. The new learner's predictions are then added to the ensemble's previous predictions, and the process \n",
    "    repeats for a specified number of iterations.\n",
    "\n",
    "    By iteratively improving the ensemble's predictions in this way, Gradient Boosting is able to reduce the bias and variance of the model, \n",
    "    leading to better accuracy and generalization performance. The learning rate hyperparameter controls the contribution of each new weak learner\n",
    "    to the ensemble, allowing for fine-tuning of the trade-off between accuracy and complexity.\n",
    "\n",
    "    \n",
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "Ans.-The Gradient Boosting algorithm builds an ensemble of weak learners in an iterative fashion. The general steps involved in building the ensemble \n",
    "     are as follows:\n",
    "        1.construct a base model.\n",
    "        2.compute the residuals.\n",
    "        3.Construct a decision tree.\n",
    "        4.Calculate similarity weight.\n",
    "        5.calculate gain to obtain the final prediction.\n",
    "        \n",
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "    algorithm?\n",
    "Ans.-The mathematical intuition behind the Gradient Boosting algorithm can be derived by minimizing a loss function with respect to the predictions of\n",
    "     the ensemble. The general steps involved in constructing the mathematical intuition are as follows.....\n",
    "        1.Define a loss function that measures the error between the predicted values and the true values of the target variable. A commonly used loss \n",
    "         function for regression problems is the mean squared error (MSE), which is the average squared difference between the predicted values and the\n",
    "         true values.\n",
    "        2.Initialize the ensemble with a constant value, typically the mean or median of the target variable.\n",
    "        3.Compute the negative gradient of the loss function with respect to the predictions of the ensemble. This represents the direction in which the \n",
    "          loss function is decreasing the fastest.\n",
    "        4.Fit a new weak learner to the negative gradient, such that the new learner can predict the negative gradient given the input features.\n",
    "        5.Calculate the optimal step size, or learning rate, for the new learner, such that the step size is small enough to avoid overfitting, but large\n",
    "          enough to make a significant contribution to the ensemble.\n",
    "        6.Add the new learner to the ensemble, with its predictions combined with the previous predictions from the ensemble.\n",
    "        7.Repeat steps 3-6 for a specified number of iterations, or until a certain threshold of performance is reached.\n",
    "        8.Finally, the predictions from all the weak learners in the ensemble are combined to obtain the final prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
