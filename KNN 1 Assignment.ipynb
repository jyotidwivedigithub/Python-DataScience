{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d50263-c69a-4db5-8b18-b074198150be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "Ans.-The K-Nearest Neighbors (KNN) algorithm is a machine learning algorithm used for classification and regression task\n",
    "\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "Ans.-K-Nearest Neighbors is the supervised machine learning algorithm used for classification and regression. It manipulates the training data \n",
    "     and classifies the new test data based on distance metrics. It finds the k-nearest neighbors to the test data, and then classification is \n",
    "     performed by the majority of class labels.\n",
    "        \n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "Ans.-The main difference between KNN classifier and KNN regressor is the type of output they produce.\n",
    "\n",
    "    KNN classifier is a supervised learning algorithm used for classification tasks. Given a new input data point, the algorithm predicts its class\n",
    "    label by looking at the class labels of its k-nearest neighbors in the training data. The predicted output is the majority class label among the\n",
    "    k-nearest neighbors. For example, if k=5 and the class labels of the 5 nearest neighbors are [1, 1, 0, 1, 0], the predicted class label would be \n",
    "    1.\n",
    "\n",
    "    KNN regressor, on the other hand, is a supervised learning algorithm used for regression tasks. Given a new input data point, the algorithm \n",
    "    predicts its output value by taking the average of the output values of its k-nearest neighbors in the training data. For example, if k=5 and the\n",
    "    output values of the 5 nearest neighbors are [4.2, 4.5, 3.9, 4.8, 4.1], the predicted output value would be (4.2+4.5+3.9+4.8+4.1)/5 = 4.3.\n",
    "    \n",
    "    \n",
    "Q4. How do you measure the performance of KNN?\n",
    "Ans.-We follow some steps tp measure performance of KNN...\n",
    "            1.Determine the number of nearest neighbours (K values).\n",
    "            2.Compute the distance between test sample and all the training samples.\n",
    "            3.Sort the distance and determine nearest neighbours based on the K-th minimum distance.\n",
    "            4.Assemble the categories of the nearest neighbours.\n",
    "            5.Utilise simple majority of the category of nearest neighbours as the prediction value of the new data object.\n",
    "            \n",
    "            \n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "Ans.-The “Curse of Dimensionality” is a tongue in cheek way of stating that there's a ton of space in high-dimensional data sets. The size of the \n",
    "     data space grows exponentially with the number of dimensions. This means that the size of your data set must also grow exponentially in order to \n",
    "     keep the same density.\n",
    "        \n",
    "        \n",
    "Q6. How do you handle missing values in KNN?\n",
    "Ans.-In KNN imputation, missing values are estimated by finding the k nearest neighbors to each sample with missing values. The missing values in the \n",
    "     sample are then imputed using the mean (or median) value of the corresponding feature in the k nearest neighbors.\n",
    "  \n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "    which type of problem?\n",
    "Ans.- KNN (k-nearest neighbors) is a popular machine learning algorithm that can be used for both classification and regression tasks. However, the \n",
    "     performance of KNN can vary depending on the specific problem and the choice of parameters. Here is a comparison of the performance of the KNN \n",
    "     classifier and regressor:\n",
    "\n",
    "    KNN Classifier:\n",
    "            1.KNN classifier is a supervised learning algorithm used for classification tasks.\n",
    "            2.KNN classifier works by finding the k-nearest neighbors to the new data point and assigning the label of the majority class among the \n",
    "             neighbors to the new data point.\n",
    "            3.KNN classifier is sensitive to the choice of the distance metric, the number of neighbors (k), and the weighting of the neighbors.\n",
    "            4.KNN classifier is better suited for problems with small to medium-sized datasets and a relatively low number of features. It can be used\n",
    "              for both binary and multi-class classification tasks.\n",
    "                \n",
    "    KNN Regressor:\n",
    "            1.KNN regressor is a supervised learning algorithm used for regression tasks.\n",
    "            2.KNN regressor works by finding the k-nearest neighbors to the new data point and averaging the target values of the neighbors to predict \n",
    "              the target value of the new data point.\n",
    "            3.KNN regressor is sensitive to the choice of the distance metric, the number of neighbors (k), and the weighting of the neighbors.\n",
    "            4.KNN regressor is better suited for problems with continuous target variables and a relatively low number of features. It can handle both \n",
    "              linear and nonlinear relationships between the features and target variables.\n",
    "                \n",
    "    Both KNN classifier and regressor have their own strengths and weaknesses, and the choice between them depends on the specific problem and the \n",
    "    characteristics of the dataset. KNN classifier is better suited for classification problems with small to medium-sized datasets and a relatively \n",
    "    low number of features, while KNN regressor is better suited for regression problems with continuous target variables and a relatively low number \n",
    "    of features. It is important to experiment with different values of k and distance metrics to find the optimal settings for a particular problem.   \n",
    "    \n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "    and how can these be addressed?\n",
    "Ans.-K-Nearest Neighbors (KNN) is a popular algorithm for both classification and regression tasks. The main strengths and weaknesses of the KNN \n",
    "     algorithm are:\n",
    "\n",
    "    Strengths:\n",
    "            1.KNN is simple to understand and easy to implement. It does not require any training as it stores all the training data.\n",
    "            2.KNN can be used for both classification and regression tasks. It is a non-parametric algorithm, which means that it does not make any \n",
    "               assumptions about the underlying data distribution.\n",
    "            3.KNN can handle both linear and nonlinear data. It can capture complex patterns in the data, especially if the number of dimensions is \n",
    "              not very high.\n",
    "\n",
    "    Weaknesses:\n",
    "            1.KNN can be computationally expensive, especially when dealing with large datasets. It requires storing all the training data and \n",
    "               computing distances between the test point and all the training points for each prediction.\n",
    "            2.KNN can be sensitive to the choice of the number of neighbors (K). If K is too small, the algorithm may overfit the training data, \n",
    "              while if K is too large, it may underfit the data.\n",
    "            3.KNN can be sensitive to the choice of distance metric. Different distance metrics may result in different predictions.\n",
    "    \n",
    "    To address the weaknesses of the KNN algorithm, the following techniques can be used:\n",
    "            1.Dimensionality reduction techniques such as PCA, t-SNE can be used to reduce the number of dimensions in the data, which can reduce the \n",
    "              computational complexity of the algorithm.\n",
    "            2.Cross-validation can be used to choose the optimal value of K. This can help to avoid overfitting or underfitting.\n",
    "            3.Different distance metrics can be tested to see which one performs best on the given dataset. Some common distance metrics include \n",
    "              uclidean distance, Manhattan distance, and cosine similarity.\n",
    "            4.Distance weighting can be used to give more weight to the closer neighbors, which can improve the accuracy of the predictions.\n",
    "            5.Ensembling techniques such as bagging or boosting can be used to combine multiple KNN models to improve the overall performance.\n",
    "            \n",
    "            \n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "Ans.-The main difference between Euclidean distance and Manhattan distance is that Euclidean distance considers the actual distance between two points \n",
    "    in a straight line, while Manhattan distance measures the distance between two points by summing the absolute differences between their \n",
    "    coordinates. In other words, Euclidean distance takes into account the direction of the points, while Manhattan distance does not. Therefore,\n",
    "    Euclidean distance is more appropriate when the data has a continuous distribution, while Manhattan distance is more suitable for discrete data.\n",
    "    \n",
    "Q10. What is the role of feature scaling in KNN?\n",
    "Ans.-Feature scaling is required to get the better performance of the KNN algorithm.Feature scaling is a method used to normalize the range of \n",
    "     independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data \n",
    "     preprocessing step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
